% !TEX root = ../main.tex
\section{Experimental Results}
\label{sec:res}
The key problem we are trying to resolve is the localization accuracy of Apriltags in noisy situations. There are two major components we want to demostrate in this paper: First, we want to characterize the effect of perceptual ambiguity and noise on Apriltags detection algorithm. Second, we want to test the resislience of our algorithm and show that it can obtain reasonable pose estimations under high level of noise. Finally, we briefly tested the runtime of the algorithm to show that it remains capable of real time detection. 

To do this, we measured the rotational and translation accuracy of the Apriltag under three different conditions: viewing angles, distances, and lighting conditions. In all three experiments, we introduced 3 different levels of simulated detection noise into our images. We placed a standard camera calibration chessboard and an Apriltag of known size on a solid planar board. The apriltag is has a fixed distance from the chessboard. This is used to compute the ground-truth pose for the tag. By using a large chessboard, we can detect the corners to a sub-pixel accuracy and compute accurate ground-truth poses unsusceptible to lighting and sensory noise.

\begin{figure*}
\centering
\subfloat[RGB Image at 75$^{\circ}$]{\includegraphics[width=\columnwidth, height=160px]{figs/result_figs/rgb_frame0001} \label{RGB image}}
\subfloat[Distrubtion]{\includegraphics[width=\columnwidth, height=170px]{figs/result_figs/result_0001} \label{RGB image}}
\hfil
\subfloat[RGB Image at 40$^{\circ}$]{\includegraphics[width=\columnwidth, height=160px]{figs/result_figs/rgb_frame0005} \label{RGB image}}
\subfloat[Close]{\includegraphics[width=\columnwidth, height=170px]{figs/result_figs/result_0005} \label{RGB image}}
\hfil
\subfloat[RGB Image at 5$^{\circ}$]{\includegraphics[width=\columnwidth, height=160px]{figs/result_figs/rgb_frame0009} \label{RGB image}}
\subfloat[Close]{\includegraphics[width=\columnwidth, height=170px]{figs/result_figs/result_0009} \label{RGB image}}
\caption{The fiducial tag plane captured by the depth sensor}
\label{fig:calib}
\end{figure*}

\subsection{Viewing Angle}
The low localization accuracy caused by the perceptual ambiguity of the Apriltags is a non-linear function on the viewing angle of the tag. To characterize the effect, we placed the testing board on a table straight in front of the robot in a well lit room. Since the sensor is taller than the plane of the table, the robot has to slightly gazing down at it.  We rotated the testing board at a increment of 5 degrees from 0 degrees to 70 degrees. This is about the range in which the tag can be detected reliably given the camera resolution and distance. At each angle, we captured the RGB image, depth image, and detection outputs from the Apriltag algorithm. 

For each captured data collection, we introduced 3 levels of Gaussian noise of $\sigma = 0.2$, $\sigma = 0.5$, $\sigma = 1$  to the data and computed for the resulting tag pose. This is repeated for $1000$ trails at each noise level and the errors are computed for each trial. Figure [7] shows some of the results. 

As the result shown in figure 7, the viewing angle has large effect on the rotation error. As we expected, the empirical results show a very clear bimodal distribution for the detected pose at different viewing angles. The depth-sensor fused algorithm vastly outperforms the previous algorithm as it is not affected by the perceptual ambiguities. The small amount of the noise introduced to the data only cause a small rotational change around the true pose of the tag. In Figure[8], we threshold all the poses based on their rotational errors and plotted the percentage of unacceptable poses at each viewing point. One important observation from the data is that, at a given angles, the magnitude of noise above a threshold has little effect on the localization accuracy. **Explain more here after running more experiments
\subsection{Distance}
In additional to the viewing angle, we captured the images at different distances away from the camera. We moved the testing board perpendicular to the sensor at 10 cm intervals. 

The relationship between the distance and localization accuracy is much more apparent. As the tag moves further away from the sensor, the number of pixels on the tag decreases. The perspective ambiguity becomes more apparent when there is only a small patch of pixels in the tag. 
\subsection{Lighting}
Based on our previous experiences, we observed that poor lighting condition is a large contributing factor to sensory noise. We tested the effects of scene lighting on our detection process by controlling the illumination in the room. We captured the pictures under three different illuminations: dark, normal, and bright. 

The Kinect sensor automatically adjusts the exposure settings to compensate for the low lighting. The pictures captured in the dark rooms still appears bright but much more grainy and apparent Gaussian noise in the image. One advantage of using RGBD sensor is that depth sensor and RGB sensor works optimally under different illuminations. In the dark setting, the depth sensor performs well. 