% !TEX root = ../main.tex
\section{Experimental Results}
\label{sec:res}
The key problem we are trying to resolve is the localization accuracy of Apriltags in noisy situations. There are two major components we want to demostrate in this paper: first, we want to characterize the effect of perceptual ambiguity and noise on the Apriltag detection algorithms. Second, we want to test the resilience of our algorithm and show that it can obtain reasonable pose estimations under high level of noise. Finally, we briefly tested the runtime of the algorithm to show that it remains capable of real time detection. 

In our experiments, we measured the rotational and translational accuracy of the detections algorithms with respect to three different indepdent variables: viewing angles, distances, and lighting conditions. In all three experiments, we introduced 3 different levels of simulated detection noise into our images. We placed a standard camera calibration chessboard and an Apriltag of known size on a solid planar board. The apriltag has a fixed distance from the chessboard. This is used to compute the ground-truth pose for the tag. By using a large chessboard, we can detect the corners to a sub-pixel accuracy and compute accurate ground-truth poses unsusceptible to lighting and sensory noise.

\subsection{Viewing Angle}
The low localization accuracy caused by the perceptual ambiguity of the Apriltags is a non-linear function on the viewing angle of the tag. To characterize the effect, we placed the testing board on a table straight in front of the robot in a well lit room. Since the sensor is taller than the plane of the table, the robot has to slightly gazing down at it.  We rotated the testing board at a increment of 5 degrees from 0 degrees to 70 degrees. This is about the range in which the tag can be detected reliably given the camera resolution and distance. At each angle, we captured the RGB image, depth image, and detection outputs from the Apriltag algorithm. 

For each captured data collection, we introduced 3 levels of Gaussian noise of $\sigma = 0.2$, $\sigma = 0.5$, $\sigma = 1$  to the RGB image and computed the resulting tag pose. This is repeated for $1000$ trails at each noise level and the errors are computed for each trial. Figure [7] shows some of the results. 
\begin{figure}
\centering
\includegraphics[width=\columnwidth]{figs/viewing_angle_fig1}
\caption{Viewing Angle vs Error Percentage under different simulated noise level. The new RGBD based algorithm can resist noise in the RGB image and it vastly outperforms the original algorithm.
\scnote{This is not immediately important but these images should be PDF/EPS and not PNG. Also the figure organization could be better}}
\end{figure}

As the result shown in figure 7 indicates, the viewing angle has large effect on the rotation error. As we expected, the emperical results show a very clear bimodel distribution for the Apriltags at different viewing angles. The depth-sensor fused algorithm vastly outperforms the previous algorithm as it is not affected by the perceptual ambiguities. The small amount of the noise introduced to the data only cause a small rotational change around the true pose of the tag. In Figure[8] \scnote{I assume you know your figure numbering is messed up}, we thresholded all the poses based on their rotational errors and ploted the percentage of unacceptable poses at each viewing point. One interesting observation from the data is that, at most viewing angles, the magnitude of noise above a certain threshold has little effect on the locationlization accuracy. At most viewing angles, relatively small noises casuses a signficant accuracy decrease. 
\subsection{Distance}
In additional to the viewing angle, we caputred the images at different distances away from the camera. We moved the testing board perpendiualr to the sensor. 

The relationship between the distance and localization accuarcy is much more apparent. As the tag moves further away from the sensor, the number of pixels on the tag decreases. The perspective ambiguity becomes more apparent when there is only a small patch of pixels in the tag. 
\subsection{Lighting}
In the unsimulated environemnt, poor lighting is a large contributing factor of sensory noise. We tested the effect of the lights on our detection process by controling the background lighting. We captured the pictures under three different lighting conditions: dark, normal, and highly exposed. 

The Kinect sensor automatically adjusts the exposure settings to compensate for the low lighting. The pictures captured in the dark rooms still appears bright but much more grainy and apparent Gaussian noise in the image. Depth sensor and RGB sensor works optimally under different lighting conditions. In the dark setting, the depth sensor performs well. 


\begin{figure*}
\centering
\subfloat[RGB Image at 75$^{\circ}$]{\includegraphics[width=\columnwidth, height=160px]{figs/result_figs/rgb_frame0001} \label{RGB image}}
\subfloat[Distrubtion]{\includegraphics[width=\columnwidth, height=170px]{figs/result_figs/result_0001} \label{RGB image}}
\hfil
\subfloat[RGB Image at 40$^{\circ}$]{\includegraphics[width=\columnwidth, height=160px]{figs/result_figs/rgb_frame0005} \label{RGB image}}
\subfloat[Close]{\includegraphics[width=\columnwidth, height=170px]{figs/result_figs/result_0005} \label{RGB image}}
\hfil
\subfloat[RGB Image at 5$^{\circ}$]{\includegraphics[width=\columnwidth, height=160px]{figs/result_figs/rgb_frame0009} \label{RGB image}}
\subfloat[Close]{\includegraphics[width=\columnwidth, height=170px]{figs/result_figs/result_0009} \label{RGB image}}
\caption{The fiducial tag plane captured by the depth sensor}
\label{fig:calib}
\end{figure*}
