\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{fiala2004artag}
\citation{olson2011apriltag}
\citation{bergamasco2011rune}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}{section.1}}
\newlabel{sec:intro}{{I}{1}{Introduction}{section.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Robot about to execute a manipulation task and rearrange the objects on the table. Apriltags are used to find the poses of targeted objects in the scene but the robot ultimately fails to gasp the rectangular prism because the orientation of its pose is wrong.\relax }}{1}{figure.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:table_clearing}{{1}{1}{Robot about to execute a manipulation task and rearrange the objects on the table. Apriltags are used to find the poses of targeted objects in the scene but the robot ultimately fails to gasp the rectangular prism because the orientation of its pose is wrong.\relax }{figure.caption.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces An overview of our purposed sensor fusion pipeline. We implicitly generate pose estimations from RGB and depth sensor observations and combine them according to their uncertainty distribution.\relax }}{1}{figure.caption.2}}
\newlabel{fig:pipeline}{{2}{1}{An overview of our purposed sensor fusion pipeline. We implicitly generate pose estimations from RGB and depth sensor observations and combine them according to their uncertainty distribution.\relax }{figure.caption.2}{}}
\citation{grest2009comparison}
\citation{besl1992method}
\citation{gedik2015rgbd,assa2014robust}
\citation{naimark2002circular}
\citation{bergamasco2011rune}
\citation{rice2006analysing}
\citation{fiala2004artag}
\citation{kato2002artoolkit}
\citation{olson2011apriltag}
\citation{wang2016apriltag}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Different types of popular fiducial tags. ARToolkit, ARTags, and AprilTags are square tags with black borders. RUNE-tags and Intersense use different circle features as landmarks\relax }}{2}{figure.caption.3}}
\newlabel{fig:tags}{{3}{2}{Different types of popular fiducial tags. ARToolkit, ARTags, and AprilTags are square tags with black borders. RUNE-tags and Intersense use different circle features as landmarks\relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {ARToolkit}}}{2}{subfigure.3.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {ARTag}}}{2}{subfigure.3.2}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {AprilTag}}}{2}{subfigure.3.3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {RUNE-Tag}}}{2}{subfigure.3.4}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(e)}{\ignorespaces {Intersense}}}{2}{subfigure.3.5}}
\@writefile{toc}{\contentsline {section}{\numberline {II}Related Work}{2}{section.2}}
\newlabel{sec:related}{{II}{2}{Related Work}{section.2}{}}
\newlabel{fig:render}{{4a}{2}{Subfigure 4a}{subfigure.4.1}{}}
\newlabel{sub@fig:render}{{(a)}{a}{Subfigure 4a\relax }{subfigure.4.1}{}}
\newlabel{fig:sketch}{{4b}{2}{Subfigure 4b}{subfigure.4.2}{}}
\newlabel{sub@fig:sketch}{{(b)}{b}{Subfigure 4b\relax }{subfigure.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The ambiguity effect can be demonstrated with two rendered cubes in the perspective view. The two cubes are rotated such that two faces are interlaced. The red square in \ref  {fig:render} is a simulated projection of a square tag. The red circular regions denote the region of potential corner detection in a noisy scene. \ref  {fig:sketch} is a sketch of the potential resulting 2D projection. The pose can converge to either one of the two faces.\relax }}{2}{figure.caption.4}}
\newlabel{fig:cube}{{4}{2}{The ambiguity effect can be demonstrated with two rendered cubes in the perspective view. The two cubes are rotated such that two faces are interlaced. The red square in \ref {fig:render} is a simulated projection of a square tag. The red circular regions denote the region of potential corner detection in a noisy scene. \ref {fig:sketch} is a sketch of the potential resulting 2D projection. The pose can converge to either one of the two faces.\relax }{figure.caption.4}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{2}{subfigure.4.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{2}{subfigure.4.2}}
\citation{hartley2003multiple,zhang2005general}
\citation{dementhon1992exact}
\citation{haralick1994review}
\citation{horaud1989analytic}
\newlabel{fig:simulation_rotation}{{5a}{3}{Subfigure 5a}{subfigure.5.1}{}}
\newlabel{sub@fig:simulation_rotation}{{(a)}{a}{Subfigure 5a\relax }{subfigure.5.1}{}}
\newlabel{fig:simulation_translation}{{5b}{3}{Subfigure 5b}{subfigure.5.2}{}}
\newlabel{sub@fig:simulation_translation}{{(b)}{b}{Subfigure 5b\relax }{subfigure.5.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Simulation results of Apriltag pose estimation under different noise level. In both simulations, four corners are projected onto a rendered image with some noise then computed their poses in each trail. In \ref  {fig:simulation_rotation}, corners are projected to the center of the camera 0.8 meters away and rotated from $0^{\circ }$ to $90^{\circ }$. In \ref  {fig:simulation_translation}, the corners are projected at $40^{\circ }$ and moved from 0.6 meters to 1.8 meters. We sampled the poses from 10,000 trials for each experiment and computed the percentage of unacceptable poses base on a threshold.\relax }}{3}{figure.caption.5}}
\newlabel{fig:simulation_results}{{5}{3}{Simulation results of Apriltag pose estimation under different noise level. In both simulations, four corners are projected onto a rendered image with some noise then computed their poses in each trail. In \ref {fig:simulation_rotation}, corners are projected to the center of the camera 0.8 meters away and rotated from $0^{\circ }$ to $90^{\circ }$. In \ref {fig:simulation_translation}, the corners are projected at $40^{\circ }$ and moved from 0.6 meters to 1.8 meters. We sampled the poses from 10,000 trials for each experiment and computed the percentage of unacceptable poses base on a threshold.\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{3}{subfigure.5.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{3}{subfigure.5.2}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Challenges}{3}{section.3}}
\newlabel{sec:problem}{{III}{3}{Challenges}{section.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Approach}{3}{section.4}}
\newlabel{sec:approach}{{IV}{3}{Approach}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-A}}Depth Plane Fitting}{3}{subsection.4.1}}
\citation{pathak2010uncertainty}
\citation{nguyen2012modeling}
\citation{pathak2010uncertainty}
\citation{eggert1997estimating}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces An abstract visualization of the optimization constraints. The blue curve is the initial pose estimation obtained from the depth plane. The red curves are the ambiguous poses from the RGB image. We constrained the region of optimization based on how well we fit the depth plane.\relax }}{4}{figure.caption.6}}
\newlabel{fig:optimization}{{6}{4}{An abstract visualization of the optimization constraints. The blue curve is the initial pose estimation obtained from the depth plane. The red curves are the ambiguous poses from the RGB image. We constrained the region of optimization based on how well we fit the depth plane.\relax }{figure.caption.6}{}}
\newlabel{eq:gaussian_noise}{{1}{4}{Depth Plane Fitting}{subsection.4.1}{}}
\newlabel{eq:gaussian_noise}{{2}{4}{Depth Plane Fitting}{subsection.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-B}}Initial Pose Estimation}{4}{subsection.4.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces The pose of the Apriltag visualized in RViz computed using the original library VS our RGBD fused method.\relax }}{4}{figure.caption.7}}
\newlabel{fig:result_compare}{{7}{4}{The pose of the Apriltag visualized in RViz computed using the original library VS our RGBD fused method.\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {RGB}}}{4}{subfigure.7.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {RGBD}}}{4}{subfigure.7.2}}
\newlabel{eq:rigid_body}{{3}{4}{Initial Pose Estimation}{figure.caption.8}{}}
\newlabel{fig:exp_setup}{{8a}{5}{Subfigure 8a}{subfigure.8.1}{}}
\newlabel{sub@fig:exp_setup}{{(a)}{a}{Subfigure 8a\relax }{subfigure.8.1}{}}
\newlabel{fig:bimodal}{{8b}{5}{Subfigure 8b}{subfigure.8.2}{}}
\newlabel{sub@fig:bimodal}{{(b)}{b}{Subfigure 8b\relax }{subfigure.8.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces An example of the experimental setup in \ref  {fig:exp_setup}. Groundtruth is computed from a large chessboard where the relative transformation to the tag is known. Each data collection, shown in \ref  {fig:bimodal}, is ran through $1000$ trails and pose errors are measured.\relax }}{5}{figure.caption.8}}
\newlabel{fig:angle_result}{{8}{5}{An example of the experimental setup in \ref {fig:exp_setup}. Groundtruth is computed from a large chessboard where the relative transformation to the tag is known. Each data collection, shown in \ref {fig:bimodal}, is ran through $1000$ trails and pose errors are measured.\relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {RGB image at 60$^{\circ }$}}}{5}{subfigure.8.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Rotation errors across 1000 trials}}}{5}{subfigure.8.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-C}}Pose Refinement}{5}{subsection.4.3}}
\newlabel{eq:reprojection}{{10}{5}{Pose Refinement}{subsection.4.3}{}}
\newlabel{eq:refinement}{{13}{5}{Pose Refinement}{subsection.4.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Viewing Angle vs Error Percentage under different simulated noise level. The new RGBD based algorithm can resist noise in the RGB image and it vastly outperforms the original algorithm.\relax }}{5}{figure.caption.9}}
\newlabel{fig:viewing_result}{{9}{5}{Viewing Angle vs Error Percentage under different simulated noise level. The new RGBD based algorithm can resist noise in the RGB image and it vastly outperforms the original algorithm.\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Experimental Results}{5}{section.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Distance vs Error Percentage. Data are captured at a $10$ cm increment from $65$ cm to $185$ cm.\relax }}{6}{figure.caption.10}}
\newlabel{fig:distance_result}{{10}{6}{Distance vs Error Percentage. Data are captured at a $10$ cm increment from $65$ cm to $185$ cm.\relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {V-A}}Viewing Angle}{6}{subsection.5.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {V-B}}Distance}{6}{subsection.5.2}}
\newlabel{fig:dark}{{11a}{6}{Subfigure 11a}{subfigure.11.1}{}}
\newlabel{sub@fig:dark}{{(a)}{a}{Subfigure 11a\relax }{subfigure.11.1}{}}
\newlabel{fig:dim}{{11b}{6}{Subfigure 11b}{subfigure.11.2}{}}
\newlabel{sub@fig:dim}{{(b)}{b}{Subfigure 11b\relax }{subfigure.11.2}{}}
\newlabel{fig:normal}{{11c}{6}{Subfigure 11c}{subfigure.11.3}{}}
\newlabel{sub@fig:normal}{{(c)}{c}{Subfigure 11c\relax }{subfigure.11.3}{}}
\newlabel{fig:bright}{{11d}{6}{Subfigure 11d}{subfigure.11.4}{}}
\newlabel{sub@fig:bright}{{(d)}{d}{Subfigure 11d\relax }{subfigure.11.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Apriltags captured by Kinect V2 under different levels of illumination. The RGB sensor dynamically adjust the exposure time to compensate for low lighting. In \ref  {fig:dark}, the image is captured outside of Kinect's adjustable range and the pixels are underexposed. In \ref  {fig:dim}, the long exposure time introduced noticeable noise to the image. \relax }}{6}{figure.caption.11}}
\newlabel{fig:illumination_tag}{{11}{6}{Apriltags captured by Kinect V2 under different levels of illumination. The RGB sensor dynamically adjust the exposure time to compensate for low lighting. In \ref {fig:dark}, the image is captured outside of Kinect's adjustable range and the pixels are underexposed. In \ref {fig:dim}, the long exposure time introduced noticeable noise to the image. \relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Dark}}}{6}{subfigure.11.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Dim}}}{6}{subfigure.11.2}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Normal}}}{6}{subfigure.11.3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {Bright}}}{6}{subfigure.11.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Illumination vs Error Percentage. Data are captured at $65$ cm away from the camera at a $40$ degree angle.\relax }}{6}{figure.caption.12}}
\newlabel{fig:lighting_result}{{12}{6}{Illumination vs Error Percentage. Data are captured at $65$ cm away from the camera at a $40$ degree angle.\relax }{figure.caption.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {V-C}}Lighting}{6}{subsection.5.3}}
\citation{alvartrack}
\bibstyle{ieeetr}
\bibdata{bib/references}
\bibcite{fiala2004artag}{{1}{}{{}}{{}}}
\bibcite{olson2011apriltag}{{2}{}{{}}{{}}}
\bibcite{bergamasco2011rune}{{3}{}{{}}{{}}}
\bibcite{grest2009comparison}{{4}{}{{}}{{}}}
\bibcite{besl1992method}{{5}{}{{}}{{}}}
\bibcite{gedik2015rgbd}{{6}{}{{}}{{}}}
\bibcite{assa2014robust}{{7}{}{{}}{{}}}
\bibcite{naimark2002circular}{{8}{}{{}}{{}}}
\bibcite{rice2006analysing}{{9}{}{{}}{{}}}
\bibcite{kato2002artoolkit}{{10}{}{{}}{{}}}
\bibcite{wang2016apriltag}{{11}{}{{}}{{}}}
\bibcite{hartley2003multiple}{{12}{}{{}}{{}}}
\bibcite{zhang2005general}{{13}{}{{}}{{}}}
\bibcite{dementhon1992exact}{{14}{}{{}}{{}}}
\bibcite{haralick1994review}{{15}{}{{}}{{}}}
\bibcite{horaud1989analytic}{{16}{}{{}}{{}}}
\bibcite{pathak2010uncertainty}{{17}{}{{}}{{}}}
\bibcite{nguyen2012modeling}{{18}{}{{}}{{}}}
\bibcite{eggert1997estimating}{{19}{}{{}}{{}}}
\bibcite{alvartrack}{{20}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Average pose errors compared with \textit  {ar\_track\_alvar} package.\relax }}{7}{figure.caption.13}}
\newlabel{fig:alvartrack}{{13}{7}{Average pose errors compared with \textit {ar\_track\_alvar} package.\relax }{figure.caption.13}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Rotation Error}}}{7}{subfigure.13.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Translation Error}}}{7}{subfigure.13.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {V-D}}Benchmark Against ar\_track\_alvar}{7}{subsection.5.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {V-E}}Computation Time}{7}{subsection.5.5}}
\@writefile{toc}{\contentsline {section}{\numberline {VI}Conclusion}{7}{section.6}}
\newlabel{sec:conclusion}{{VI}{7}{Conclusion}{section.6}{}}
